---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hi, thanks for stopping by.

I am **Zonglin Yang**, a third-year Ph.D. candidate at Nanyang Technological University supervised by [Erik Cambria](https://sentic.net/erikcambria/). I also work closely with [Soujanya Poria](https://soujanyaporia.github.io/). Before NTU, I obtained my master's degree at Cornell University, supervised by [Claire Cardie](https://www.cs.cornell.edu/home/cardie/) and [Xinya Du](https://xinyadu.github.io/), and my bachelor's degree at Huazhong University of Science and Technology, supervised by [Xinggang Wang](https://xwcv.github.io/). I have interned at Microsoft Research in NLC group mentored by [Li Dong](http://dong.li/). I'm also a visiting student at Princeton University host by [Mengdi Wang](https://mwang.princeton.edu/).

My representative works are the MOOSE and MOOSE-Chem series. 

*Like the moose that ventures into uncharted wilderness, MOOSE & MOOSE-Chem series explores the untamed landscape of scientific hypotheses to uncover hidden insights.*

I am open to academic collaborations and please drop me an email (<zonglin.yang@ntu.edu.sg>) if you are interested in collaborating with me.
<!--**I am actively seeking internship opportunities in 2024. If you have any openings available, kindly reach out to me via email (<zonglin.yang@ntu.edu.sg>).**-->
<!--**I will be on the job market in June 2025! Feel free to reach out if youâ€™re interested in my research or have relevant opportunities.**-->
<!--I will be in the academic and industrial job market in June 2025! If you have any openings available, kindly reach out to me via email (<zonglin.yang@ntu.edu.sg>).-->


Research
======
My current research interests are:  
* LLMs for Scientific Discovery
  * [MOOSE](https://arxiv.org/abs/2309.02726) (ACL 2024)
    * The first work showing that LLMs can be leveraged to generate novel and valid scientific hypothesis.
  * [MOOSE-Chem](https://arxiv.org/abs/2410.07076) (ICLR 2025)
    * Provide a mathematically proved theoretical foundation on automated scientific hypothesis discovery.
    * The first work showing that LLMs can rediscover the main innovations of many research hypotheses published in Nature or Science.
  * [MOOSE-Chem2](https://arxiv.org/abs/2505.19209) (NeurIPS 2025)
    * Introduce the task of *fine-grained scientific hypothesis discovery*, aiming for experimentally actionable hypothesis.
    * Frame the task as an optimization problem, and propose hierarchical heuristic search that can theoretically smoothen the optimization landscape to reach to better local optimum.
  * [MOOSE-Chem3](https://arxiv.org/abs/2505.17873)
    * Introduce the task of *experiment-guided ranking*, which bridges automated scientific hypothesis discovery and experimental feedback.
    * Propose an experimental simulator that enables scalable research of *experiment-guided ranking* without relying on real wet-lab experiments.
  * [ResearchBench](https://arxiv.org/pdf/2503.21248)
    * The first large-scale benchmark for evaluating LLMs with a sufficient set of sub-tasks of scientific discovery: inspiration retrieval, hypothesis composition, and hypothesis ranking.
    * It suggests that LLMs can serve as research hypothesis mines, with stronger LLMs acting as richer mines and greater inference compute enabling more miners.
  * [NoveltyBench](https://arxiv.org/abs/2505.24615)
    * The first benchmark for evaluating the novelty of a scientific hypothesis.
  * [Survey](https://arxiv.org/pdf/2501.04306)
    * The first comprehensive survey on how LLMs can assist scientific research.
<!--* Alignment of LLMs.-->
* Reasoning & Knowledge in Natural Language Processing
  * [Inductive Reasoning](https://aclanthology.org/2024.eacl-long.13/) (EACL 2024)
    <!--* The first work on language models for generative inductive reasoning.-->
  * [Logical Reasoning Survey](https://arxiv.org/pdf/2303.12023.pdf)
  * [Case-based Reasoning](https://aclanthology.org/2023.eacl-main.255/) (EACL 2023)
  * [Temporal Commonsense Reasoning](https://aclanthology.org/2020.findings-emnlp.302/) (EMNLP 2020)

<!-- More specifically about my past research: -->

<!-- (1) Bridging research between classic AI (Knowledge Representation and Reasoning) and Natural Language Processing, e.g., [Inductive Reasoning](https://aclanthology.org/2024.eacl-long.13/) and [Case-based Reasoning](https://aclanthology.org/2023.eacl-main.255/). -->

<!-- (2) A [Further Work on Inductive Reasoning](https://arxiv.org/abs/2309.02726) that leverages LLM agents to generate novel and valid social science hypotheses to assist scientist. -->

<!-- (3) A further work on [Chemistry Scientific Discovery](https://arxiv.org/abs/2410.07076) that can rediscover the chemistry hypotheses published in Nature and Science. -->

<!-- (3) A [Survey on Logical Reasoning](https://arxiv.org/pdf/2303.12023.pdf) over Natural Language as Knowledge Representation.  -->
<!-- It surveys papers under a new paradigm of logical reasoning that uses natural language as knowledge representation and PLMs as reasoners. In contrast, in the classic AI research of logical reasoning, formal language is used as knowledge representation, and symbolic methods are used as reasoners.   -->

<!-- ![Alt text](https://github.com/ZonglinY/ZonglinY.github.io/blob/master/images/LRNL.pdf) -->

<!--(4) Improving [Temporal Commonsense Reasoning](https://aclanthology.org/2020.findings-emnlp.302/) via distant supervision.-->



<!-- My primary research goal is to apply Deep Learning for Natural Language Processing and develop **Language Technology for All**. To achievethis goal and make language technology accessible in most peopleâ€™s lives, I identify two major research topics that Iâ€™m interested in: **efficiency** and **trustworthiness** of NLP models. Efficiency involves both the amount of **computation** and **data** required for (pre-)training and using NLP models. Trustworthiness involves the **interpretability**, **fairness**, and **robustness** with respect to adversarial attacks and out-of-distribution samples.  

Specifically I am interested in the following research topics:  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Natural language generation, creative text generation, evaluation for NLG models.**  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Commonsense reasoning and knowledge-based reasoning.**  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Robust NLP models for OOD samples and reducing spurious dataset biases.**  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Interpretability, explainability, biases, and fairness for NLP models.**  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Green NLP, Low resource NLP, and Learning NLP models from high-level supervision**  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Multi-modality**   -->
<!-- I am open to academic collaborations and please drop me an email if you are interested in collaborating with me.   -->

News
======
\[2025.09]. [MOOSE-Chem2](https://arxiv.org/abs/2505.19209) is accepted to **NeurIPS 2025**. Thanks to all my collaborators!

\[2025.09]. I'll host a **tutorial** on [*Agentic AI for Scientific Discovery*](https://agent4sd.io/) at **AAAI 2026** on January 21th, stay tuned! 

\[2025.05]. I'll host a **tutorial** on *Critical Works on LLMs for Scientific Discovery* at [AI4X](https://ai4x.cc/tutorials/) on July 7th!

<!--\[2025.03]. Our preprint of [ResearchBench](https://arxiv.org/pdf/2503.21248) is out.  -->

\[2025.03]. I will take an **invited talk** at ICLR 2025 [Agentic AI4S workshop](https://iclragenticai.github.io/) on [MOOSE-Chem](https://arxiv.org/abs/2410.07076v4) on April 27th! The recording can be found in [here](https://iclr.cc/virtual/2025/33129). 

\[2025.02]. Got one paper accepted to **CVPR 2025**. Congrats to Di, Junxian, Jingdi, and Xunzhi!

\[2025.01]. I'm invited as an **Area Chair** in ACL Rolling Review (ARR)!

\[2025.01]. [MOOSE-Chem](https://arxiv.org/abs/2410.07076) is accepted to **ICLR 2025**. Thanks to all my collaborators!

<!--\[2025.01]. Our preprint of [Survey on LLMs for Scientific Research](https://arxiv.org/pdf/2501.04306) is out.  -->

<!--\[2024.10]. Our preprint of [Chemistry Scientific Discovery](https://arxiv.org/abs/2410.07076) is out.-->

\[2024.07]. [MOOSE](https://arxiv.org/abs/2309.02726) has won the **Best Poster Award** in ICML 2024 [AI4Science workshop](https://ai4sciencecommunity.github.io/icml24/award.html)! 

<!--\[2024.06]. In summary, our [LLM & Scientific Discovery](https://arxiv.org/abs/2309.02726) paper will be presented at ICML 2024 [AI4Science workshop](https://ai4sciencecommunity.github.io/icml24.html), IJCAI 2024 [AI4Research workshop](https://ai4research.github.io/), ACL 2024 [NLRSE workshop](https://nl-reasoning-workshop.github.io/), ACL 2024 [KnowledgeLM workshop](https://knowledgeable-lm.github.io/), and [AI4Science and Nobel Turing Challenge Initiative Conference](https://ai4science.sg/ai4sci%2Fntci-conf#f870436a-eaa4-414b-a8c6-b89ba727fdb5)! So you know where to find meðŸ˜†ðŸ˜†!-->

\[2024.05]. [MOOSE](https://arxiv.org/abs/2309.02726) is accepted to **ACL 2024**. Thanks to all my collaborators!  

<!-- \[2024.01]. I will physically attend EACL 2024 at Malta! Please let me know if you want to chat with me! -->

<!--\[2024.04]. I'm invited to be an area chair of ICML 2024 [AI4Science workshop](https://ai4sciencecommunity.github.io/icml24.html)!-->

\[2024.04]. I will take an **invited talk** at IJCAI 2024 [AI4Research workshop](https://ai4research.github.io/) on [MOOSE](https://arxiv.org/abs/2309.02726) on August 5th!  

\[2024.01]. Got one paper accepted to **EACL 2024**. Thanks to all my collaborators!  

<!-- \[2023.12]. I will physically attend EMNLP 2023 at Singapore! Please let me know if you want to chat with me! -->

\[2023.10]. Got one paper accepted to **EMNLP 2023**. Congrats to Wei!

<!--\[2023.09]. Our preprint of [LLM & Scientific Discovery](https://arxiv.org/abs/2309.02726) is out.-->

\[2023.06]. I will take an **invited talk** at ICCBR 2023 [TMG workshop](https://recap.uni-trier.de/workshops/tmg-2023/) on our [Case-based Reasoning](https://aclanthology.org/2023.eacl-main.255/) paper on July 17th!  

<!--\[2023.05]. Our [Survey on Logical Reasoning](https://arxiv.org/pdf/2303.12023.pdf) will be present at ACL 2023 [NLRSE workshop](https://nl-reasoning-workshop.github.io/) (in a non-archival way) on July 13th!-->

\[2023.05]. Got one paper accepted to **ACL 2023**. Congrats to Jinjie!

<!-- \[2023.05]. Our inductive reasoning [paper](https://aclanthology.org/2024.eacl-long.13/) receives very [positive comments](https://docs.google.com/document/d/1wBUL8f3HtR_DN8loGR5IhDjQawrxOykv22-NW-dzZPY/edit?usp=sharing) from meta-reviewer in ACL 2023 but is rejected! (ACL 2023 excitement score pre-rebuttal: 4, 4, 2.5; after rebuttal: 4, 3.5, 2.5; ACL meta agrees with our rebuttal against r3; AAAI 2023 score: 7, 7, 4, 4) -->

<!-- \[2023.04]. ~~I'm planning to physically attend EACL 2023!~~ Let me know if you want to chat with me ~~in Dubrovnik! (if I can successfully get the visa)~~ (No I can't get the visa!) -->

\[2023.04]. Our EACL 2023 [paper](https://aclanthology.org/2023.eacl-main.255/) will have an **oral** presentation!  

\[2023.01]. Got one paper accepted to **EACL 2023**. Thanks to all my collaborators!  

\[2020.10]. Got one paper accepted to **EMNLP 2020 (findings)**. Thanks to all my collaborators!  


Academic Services
======
Area Chair:  
<!--ICML 2024 AI4Science Workshop-->
* ARR

Conference Reviewer: 
* ARR, NeurIPS 2025, COLM 2025, ICLR 2025, NLPCC 2024, COLM 2024, COLING 2024, EMNLP 2023, ACL 2023, EMNLP 2022, COLING 2022  

<!-- Journal Reviewer: 
* IEEE Transactions on Affective Computing
* Knowledge-Based Systems
* Information Fusion
* Artificial Intelligence Review
* Cognitive Computation  -->

Student Volunteer:  
* EMNLP 2023  


<!-- <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=YRGrykBPpahx3PGSEvSxEgxL4CMmQCpR10FpVR2MVTE&cl=ffffff&w=a"></script>
 -->
 <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=304&t=n&d=YRGrykBPpahx3PGSEvSxEgxL4CMmQCpR10FpVR2MVTE&co=2d78ad&cmo=3acc3a&cmn=ff5353&ct=ffffff'></script>
<!-- \[2018.12\] Start my research internship at at [NLC Group @ Microsoft Research Asia](https://www.microsoft.com/en-us/research/group/natural-language-computing/), advised by Dr. Tao Ge.  

\[2018.8\] Start my Master study at NLSDE Lab in Beihang University, advised by Prof. Ke Xu   -->

<!-- Personal information
------
I am a big fan of Harry Potter, Real Madrid, and Cristiano Ronaldo. I enjoy reading books (especially sci-fictions) and playing games (including FIFA, League of Legends, etc.) in my free time. -->
